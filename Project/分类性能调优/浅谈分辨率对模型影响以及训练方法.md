## 一、前言
最近几个人在讨论模型训练的时候，提到了一个尺度对于模型的影响以及训练方法的收益，因此花了点时间，简单做了几组实验，整理一下结论。

## 二、基础配置
本文的实验均是采用固定的配置结构以及同一套code实现，每次实验只改变分辨率等变量因素，保证实验合理性。

代码实现可以参考我的这个git repo
https://github.com/FlyEgle/imageclassification

**模型**：ResNet50
**数据集**：ImageNet1k-128w
**数据增强**：RandomResizeCrop+RandomFlip
**优化器**：SGD+momentum
**学习率**：$lr=0.1\times \frac{\textup{batchsize}}{256}$
**学习率衰减**：cosineLr
**混合精度**: yes

## 三、实验

本次实验分成Pretrain和Finetune两大部分，实验过程和结果如下：
### 3.1 Pretrain
pretrain这里分成两组实验来做，第一组无任何pretrain，第二组用不同的方法load pretrain。
#### 3.1.1 Training from Sketch
第一组实验，总计4次对比实验，分别实验在不同的分辨率的情况下R50的表现，训练周期均为90个epoch, warmup 5个epoch，训练配置保持一致，结果如下：

|模型|分辨率|batchsize|acc@top1|
|:---:|:---:|:---:|:---:|
|R50|224|1024|76.548%|
|R50|320|1024|77.698%|
|R50|416|1024|<font color=Red>78.026%</font>|
|R50|448|1024|77.826%|

可以看到，随着分辨率的增加，模型精度随之上升，但是在448分辨率的情况下，训练的精度反而没有416高，说明不加载pretrain的情况下，想要按原始的训练配置来提升精度，在更高的分辨率下收益不会有太大提升。

#### 3.1.2 Training from Progressive
第二组实验，总计3次对比实验，分别是更大的分辨率先load小分辨率的权重后在按相同配置进行训练，由于batchsize在1k，所以我没有舍弃warmup，3.2的配置和3.1的配置是保持一致的，结果如下:

|模型|分辨率|batchsize|acc@top1|
|:---:|:---:|:---:|:---:|
|R50|224->320|1024|78.052%|
|R50|320->416|1024|<font color=Red>78.678%</font>|
|R50|416->448|1024|78.542%|

可以看到相比第一组实验，相同分辨率下的精度均有提升，而且在448的时候，没有显著的下降。

为了验证load精度稍微差一点的pretrain是否有提升，做了如下的实验，不同的分辨率训练，但是固定load的pretrain都是224的权重，结果如下：

|模型|分辨率|batchsize|acc@top1|
|:---:|:---:|:---:|:---:|
|R50|224->320|1024|78.052%|
|R50|224->416|1024|78.572%|
|R50|224->448|1024|<font color=Red>78.722%</font>|

很惊奇的发现，在448的分辨率下，精度达到了最高。

### 3.2 Finetune
这里设计了两大组实验，分别考虑了学习率缩放和冻结block。
#### 3.2.1 Finetune with LR
前面的实验因为都是做pretrain和from sketch，所以固定了所有的配置，包括LR。这里对LR缩放做实验，探究finetune对模型精度的影响。缩放原始LR为0.1和0.01倍，去掉warmup，只训练40个epoch，结果如下：

|模型|分辨率|batchsize|LR|acc@top1|
|:---:|:---:|:---:|:---:|:---:|
|R50|224->448|1024|LR*0.1|<font color=Red>78.924%|
|R50|224->448|1024|LR*0.01|78.736%|

可以看到，不固定任何参数的时候，缩放LR，finetune相比pretrain，会有较大幅度的提升。

#### 3.2.2 Finetune with Freeze layer
上一组实验得到了LR*0.1的时候效果最好，但是没有冻结模型的任何参数，这组实验做一下上面实验的补充，R50总计有4个layer，以及conv stem和FC。FC是不会被冻结的，不然训练个寂寞了，所以调整的就是不同的layer的冻结，结果如下：

|模型|分辨率|batchsize|LR|layer|acc@top1|
|:---:|:---:|:---:|:---:|:---:|:---:|
|R50|224->448|1024|LR*0.1|conv stem+layer1|78.796%|
|R50|224->448|1024|LR*0.01|conv stem+layer1-2|<font color=red>78.822%|
|R50|224->448|1024|LR*0.01|conv stem+layer1-3|78.05%|
|R50|224->448|1024|LR*0.01|conv stem+layer1-4|74.432%|

可以看到，当冻结前两个layer的时候，模型的性能还ok，但是当开始冻结第三个layer的时候有明显的下降，全部冻结下降的更明显。

## 四、结论
- 如果同域数据，不希望提升模型的参数量的情况下，想要提升精度，可以考虑pretrain的方法。Finetune的方法虽然精度可以上来，但是bad case有时候会跟着权重继承过来。
- 数据量多，时间不够的情况下可以适当freeze前面几层layer进行finetune，浅层特征一般是共性特征，影响精度的还是底层的抽象特征。
- imagnet的数据集是属于目标性的，就是物体很明确的在图像中表示出来，如果你的任务是理解性质的或者需要隐表征的，那么最好从头训练(经验之谈)

**以上实验结论仅供参考，不能保证不同数据集结论一致，欢迎交流讨论。**

